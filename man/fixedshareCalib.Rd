% Generated by roxygen2 (4.0.1): do not edit by hand
\name{fixedshareCalib}
\alias{fixedshareCalib}
\title{Fixed-share aggregation rule with automatic tunning of the parameters}
\usage{
fixedshareCalib(y, experts, grideta = 1, gridalpha = 10^(-4:-1),
  awake = NULL, loss.type = "squareloss", loss.gradient = TRUE,
  w0 = NULL, href = 1, period = 1, trace = F)
}
\arguments{
\item{y}{A vector containing the observations
to be predicted.}

\item{experts}{A matrix containing the experts
forecasts. Each column corresponds to the predictions proposed by an expert
to predict \code{Y}. It has as many columns as there are experts.}

\item{awake}{A matrix specifying the
activation coefficients of the experts. Its entries lie in \code{[0,1]}.
Needed if some experts are specialists and do not always form and suggest
prediction.  If the expert number \code{k} at instance \code{t} does not
form any prediction of observation \code{Y_t}, we can put
\code{awake[t,k]=0} so that the mixture does not consider expert \code{k} in
the mixture to predict \code{Y_t}.  A matrix containing the activation
coefficient of the experts. Its entries lie in \code{[0,1]}.  Needed if some
experts are specialists and do not always form and suggest prediction.  If
the expert number \code{k} at instance \code{t} does not form any prediction
of observation \code{Y_t}, we can put \code{awake[t,k]=0} so that the
mixture does not consider expert \code{k} in the mixture to predict
\code{Y_t}.}

\item{grideta}{A vector containing the
initial grid of allowed learning parameters to consider. It will be extended
if its borders perform well.}

\item{gridalpha}{A vector specifying the
allowed mixing parameters considered by the aggregation rule. It will not be
changed.}

\item{loss.type}{Loss function
considered to evaluate the performance. It can be "squareloss", "mae",
"mape", or "pinballloss". See \code{\link{loss}} for more details.}

\item{loss.gradient}{A boolean. If
TRUE (default) the aggregation rule will not be directly applied to the loss
function at hand but to a gradient version of it. The aggregation rule is
then similar to gradient descent aggregation rule.}

\item{w0}{prior weights vector of the
experts.}

\item{href}{A number in \code{[1,period]}
specifying the instant in the day when the aggregation rule can update its
weights.  It should lie in the intervall \code{c(1,period)}.}

\item{period}{The number of instants in
each day.}

\item{trace}{A boolean. If TRUE, the
evolution of the aggregation rule is displayed. Usefull if the code is too
long.}
}
\value{
\item{weights }{a matrix of dimension \code{c(T,N)}, with
\code{T} the number of instances to be predicted and \code{N} the number of
experts. Each row contains the convex combination to form the predictions}
\item{prediction }{ A vector of length \code{T} that contains the
predictions outputted by the aggregation rule.  } \item{par}{the sequence of
parameters \code{eta} and \code{alpha} chosen by the aggregation rule}
\item{grideta}{the final grid of potential learning parameter \code{eta}
considered by the aggregation rule} \item{gridalpha}{the final grid of
potential mixing parameter \code{alpha} considered by the aggregation rule}
\item{loss}{the error suffered by the aggregation rule determined by
\code{loss.type}.  If \code{loss.type = 'squareloss'}, the \link{rmse} is
computed.} \item{gridloss}{errors suffered by the
\code{fixedshareHour} aggregation rule if it had picked the fixed
learning rates in \code{gridalpha} and \code{grideta}}
}
\description{
The
function \code{fixedshareCalib} performs \code{fixedshareHour}
aggregation rule with automatic calibration of the learning parameter
\code{eta} and \code{alpha} by performing an optimization on a finite grid.
See \code{fixedshare} and \code{fixedshareHour} for more
details.
}
\author{
Pierre Gaillard <pierre-p.gaillard@edf.fr>
}
\keyword{~kwd1}
\keyword{~kwd2}

